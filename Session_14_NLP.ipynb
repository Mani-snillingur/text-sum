{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e7CoAx960hwn",
        "ZVSwUT4t0dky",
        "TkGRBpmM1dlJ",
        "bFwAbHgE2dZO",
        "97CzfXdi3bIU",
        "SBf4KfWA36s3"
      ],
      "authorship_tag": "ABX9TyM6yY/Qo70YUH3Uftg9sUzM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mani-snillingur/text-sum/blob/dev1/Session_14_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date: 23/12/2023"
      ],
      "metadata": {
        "id": "e7CoAx960hwn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmiBRKy6Oo17"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfvfH5VnXm8C",
        "outputId": "65430569-ed9d-4c43-9e68-1845f60d0a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text preprocessing\n",
        "def text_preprocessing(text):\n",
        "  words = word_tokenize(text.lower())\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "YtXPz1aTX8l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"This is NLP session & Text-preprocessing is an essential step.\"\n",
        "text = \"This is NLP session & Text-preprocessing is an essential step. This is NLP session & Text-preprocessing is an essential step.\"\n",
        "processed_text = text_preprocessing(text)\n",
        "\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Processed Text: \", processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZRGP5-7ZLHn",
        "outputId": "59e991a2-1738-40f2-f7d6-1d5cdcfe673e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  This is NLP session & Text-preprocessing is an essential step. This is NLP session & Text-preprocessing is an essential step.\n",
            "Processed Text:  ['nlp', 'session', 'essenti', 'step', 'nlp', 'session', 'essenti', 'step']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "m7eSgNUgZ1mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = FreqDist(processed_text)\n",
        "print(fdist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neCQt4z-aoKO",
        "outputId": "a847d385-7d86-496d-b138-cb8955f381b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 4 samples and 8 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noise Removal\n",
        "import re\n",
        "\n",
        "\n",
        "def noise_remove(text):\n",
        "  cln_txt = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "  cln_txt = ' '.join(cln_txt.split())\n",
        "\n",
        "  return cln_txt\n",
        "\n",
        "\n",
        "text = \"This is a @ Simple Sentence with some number 12345!\"\n",
        "\n",
        "cl_txt = noise_remove(text)\n",
        "print(cl_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDfLu__ha20z",
        "outputId": "4f951db4-3188-44e6-e990-d06f5cc9df75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Simple Sentence with some number\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexicon Normalization\n",
        "# 1. Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "word = \"Playing\"\n",
        "stem = stemmer.stem(word)\n",
        "\n",
        "print(stem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OFWg9ahdG4j",
        "outputId": "03986d61-65a6-4d97-a4ce-5443d7112203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Lemmatization\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK_nBSrxeGWW",
        "outputId": "2f8946a4-cdf4-4090-a0bf-6cc3bb0b8205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"better\"\n",
        "lemma = lemmatizer.lemmatize(word, pos='a')\n",
        "\n",
        "print(lemma)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kVjILMoeWqy",
        "outputId": "636774c1-4f6d-43b1-9350-91467189389f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Object Standardization\n",
        "from datetime import datetime\n",
        "\n",
        "def date_stand(text):\n",
        "    date_pattern = re.compile(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b')\n",
        "    match_date = date_pattern.findall(text)\n",
        "    stander_txt = text\n",
        "\n",
        "    for i in match_date:\n",
        "      try:\n",
        "        date_obj = datetime.strptime(i, '%Y-%m-%d')\n",
        "        date_stand2 = date_obj.strftime('%Y-%m-%d')\n",
        "        stander_txt =  stander_txt.replace(i, date_stand2)\n",
        "\n",
        "      except ValueError:\n",
        "        pass\n",
        "\n",
        "    return stander_txt\n",
        "\n",
        "text = \"date is 2023/12/23, tommorow 24-12-2023, yeasterday 2023-12-22\"\n",
        "\n",
        "date_text = date_stand(text)\n",
        "\n",
        "print(date_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RStnfwVe0qF",
        "outputId": "91adaabf-5b61-458d-d5ce-62bf91817f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date is 2023/12/23, tommorow 24-12-2023, yeasterday 2023-12-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date: 24/12/2023"
      ],
      "metadata": {
        "id": "ZVSwUT4t0dky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to Features (Feature Engineering) -> Bag-of-Words (BoW)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\"This is a sample sentence.\", \"This sentence is the second part.\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"Bag of Words matrix:\")\n",
        "print(X.toarray())\n",
        "print(\"Feature Names:\", feature_names)"
      ],
      "metadata": {
        "id": "rNKYPeSAjtT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1658eb-5ec4-435d-c0d7-a6198095ab9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words matrix:\n",
            "[[1 0 1 0 1 0 1]\n",
            " [1 1 0 1 1 1 1]]\n",
            "Feature Names: ['is' 'part' 'sample' 'second' 'sentence' 'the' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Term Frequency-Inverse Document Frequency\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\"This is a sample sentence.\", \"This sentence is the second part.\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X.toarray())\n",
        "print(\"Feature Names:\", feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzO3bLUvxlHQ",
        "outputId": "7d1eb295-d714-466f-d377-8cea360996c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.44832087 0.         0.63009934 0.         0.44832087 0.\n",
            "  0.44832087]\n",
            " [0.33471228 0.47042643 0.         0.47042643 0.33471228 0.47042643\n",
            "  0.33471228]]\n",
            "Feature Names: ['is' 'part' 'sample' 'second' 'sentence' 'the' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embeddings -> models -> Word2Vec, GloVe\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "try:\n",
        "    word2vec_model = api.load('word2vec-google-news-300')\n",
        "except (FileNotFoundError, api.DownloadError):\n",
        "    print(\"Downloading...\")\n",
        "    word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Specify the local path and filename to save the Word2Vec model\n",
        "local_path = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "model_filename = \"word2vec_google_news.model\"\n",
        "full_path = local_path + model_filename\n",
        "\n",
        "# Save the Word2Vec model to the specified local path\n",
        "word2vec_model.save(full_path)\n",
        "\n",
        "print(f\"Word2Vec model saved to {full_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "emeqtdUfx6_O",
        "outputId": "edff13a2-eba6-49f5-c13f-8d99fb8802c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[====----------------------------------------------] 9.6% 159.3/1662.8MB downloaded"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e6d1fc7cb66a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e6d1fc7cb66a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-google-news-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gensim.downloader' has no attribute 'DownloadError'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = Word2Vec.load(full_path)"
      ],
      "metadata": {
        "id": "FHOuDNSY0Ntc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Word embeddings are powerful tools for NLP.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "word_embeddings = [loaded_model[word] for word in tokens if word in loaded_model]\n",
        "\n",
        "print(\"Word Embeddings:\")\n",
        "print(word_embeddings)"
      ],
      "metadata": {
        "id": "oqsidkCIyWIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactical Parsing\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ4ol19kyjLs",
        "outputId": "7f733352-222b-42ca-cf34-77cf619b8742"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The --> det --> fox\n",
            "quick --> amod --> fox\n",
            "brown --> amod --> fox\n",
            "fox --> nsubj --> jumps\n",
            "jumps --> ROOT --> jumps\n",
            "over --> prep --> jumps\n",
            "the --> det --> dog\n",
            "lazy --> amod --> dog\n",
            "dog --> pobj --> over\n",
            ". --> punct --> jumps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency Grammer\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} --{token.dep_}--> {token.head.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVZ9ec5JyvMd",
        "outputId": "83616f23-0be1-4739-a0ea-1ca7011b4199"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The --det--> cat\n",
            "cat --nsubj--> sat\n",
            "sat --ROOT--> sat\n",
            "on --prep--> sat\n",
            "the --det--> mat\n",
            "mat --pobj--> on\n",
            ". --punct--> sat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Tagging\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMy1O45Ly3D3",
        "outputId": "ef740405-f703-46e9-ad5e-e1a25fa8b1f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The: DET\n",
            "quick: ADJ\n",
            "brown: ADJ\n",
            "fox: NOUN\n",
            "jumps: VERB\n",
            "over: ADP\n",
            "the: DET\n",
            "lazy: ADJ\n",
            "dog: NOUN\n",
            ".: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entity Parsing/ NER\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino. It became a major tech company.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Yl6yp1y-uK",
        "outputId": "0e2a9742-37a3-4597-e1e7-8b0430e21bf0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple Inc.: ORG\n",
            "Steve Jobs: PERSON\n",
            "Cupertino: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic Modelling\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "documents = [\n",
        "    \"Natural language processing is a subfield of artificial intelligence.\",\n",
        "    \"Topic modeling helps in discovering hidden topics within a collection of documents.\",\n",
        "    \"Latent Dirichlet Allocation is a popular algorithm for topic modeling.\",\n",
        "    \"Text data preprocessing involves tasks like tokenization and removing stop words.\"\n",
        "]\n",
        "\n",
        "\n",
        "preprocessed_documents = [remove_stopwords(doc.lower()) for doc in documents]\n",
        "tokenized_documents = [doc.split() for doc in preprocessed_documents]\n",
        "dictionary = corpora.Dictionary(tokenized_documents)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_documents]\n",
        "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
        "\n",
        "topics = lda_model.print_topics(num_words=3)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdDgJ4BtzgHK",
        "outputId": "8e58c7c9-5136-4a05-9477-94bfed9fe453"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.052*\"topic\" + 0.052*\"documents.\" + 0.052*\"helps\"')\n",
            "(1, '0.047*\"tasks\" + 0.047*\"text\" + 0.047*\"like\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N-Grams\n",
        "def generate_ngrams(text, n):\n",
        "    \"\"\"\n",
        "    Generate n-grams from a given text.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "\n",
        "example_text = \"This is an example sentence for n-gram generation.\"\n",
        "\n",
        "bi_grams = generate_ngrams(example_text, 2)\n",
        "print(\"Bi-grams:\")\n",
        "print(bi_grams)\n",
        "\n",
        "\n",
        "tri_grams = generate_ngrams(example_text, 3)\n",
        "print(\"\\nTri-grams:\")\n",
        "print(tri_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WX7nUK5zri5",
        "outputId": "609effd2-0c25-4e5b-f1e4-008c3c68afb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-grams:\n",
            "['This is', 'is an', 'an example', 'example sentence', 'sentence for', 'for n-gram', 'n-gram generation.']\n",
            "\n",
            "Tri-grams:\n",
            "['This is an', 'is an example', 'an example sentence', 'example sentence for', 'sentence for n-gram', 'for n-gram generation.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification"
      ],
      "metadata": {
        "id": "TkGRBpmM1dlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "1mU0iPCZ1clk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "QOQQ9d8h1ksq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "yCQfPDX51nZ-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_vectorized, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "6DAbaRR81r9W",
        "outputId": "6cca1a78-e838-4d44-a37a-f6c1eb46742e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = classifier.predict(X_test_vectorized)\n",
        "\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7-NRG1x2BVD",
        "outputId": "94f80f2d-d426-4b50-e8e9-21ad31ddfa2c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions, target_names=newsgroups.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHiWiXru2FeS",
        "outputId": "69df334f-93ae-40a1-efe1-12b4b380e6dc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.61      0.25      0.36       151\n",
            "           comp.graphics       0.48      0.75      0.58       202\n",
            " comp.os.ms-windows.misc       0.73      0.04      0.08       195\n",
            "comp.sys.ibm.pc.hardware       0.53      0.73      0.62       183\n",
            "   comp.sys.mac.hardware       0.86      0.58      0.69       205\n",
            "          comp.windows.x       0.68      0.80      0.74       215\n",
            "            misc.forsale       0.88      0.53      0.66       193\n",
            "               rec.autos       0.87      0.63      0.73       196\n",
            "         rec.motorcycles       0.49      0.58      0.53       168\n",
            "      rec.sport.baseball       0.99      0.67      0.80       211\n",
            "        rec.sport.hockey       0.92      0.80      0.86       198\n",
            "               sci.crypt       0.59      0.77      0.67       201\n",
            "         sci.electronics       0.84      0.49      0.62       202\n",
            "                 sci.med       0.82      0.75      0.79       194\n",
            "               sci.space       0.76      0.69      0.72       189\n",
            "  soc.religion.christian       0.36      0.95      0.52       202\n",
            "      talk.politics.guns       0.80      0.55      0.65       188\n",
            "   talk.politics.mideast       0.50      0.80      0.61       182\n",
            "      talk.politics.misc       0.39      0.67      0.49       159\n",
            "      talk.religion.misc       0.67      0.04      0.08       136\n",
            "\n",
            "                accuracy                           0.62      3770\n",
            "               macro avg       0.69      0.60      0.59      3770\n",
            "            weighted avg       0.70      0.62      0.60      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Matching"
      ],
      "metadata": {
        "id": "bFwAbHgE2dZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(str1, str2):\n",
        "  set1 = set(str1.split())\n",
        "  set2 = set(str2.split())\n",
        "  intersection = len(set1.intersection(set2))\n",
        "  union = len(set1.union(set2))\n",
        "  return intersection / union if union != 0 else 0\n",
        "\n",
        "text1 = \"The quick brown fox\"\n",
        "text2 = \"A quick brown dog\"\n",
        "\n",
        "similarity_score = jaccard_similarity(text1, text2)\n",
        "\n",
        "print(f\"Jaccard Similarity: {similarity_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0tklya62bfg",
        "outputId": "ed0c5591-30cf-4c0f-fabb-a4f25f897433"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity: 0.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "text1 = \"The quick brown fox\"\n",
        "text2 = \"A quick brown dog\"\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "cosine_sim = cosine_similarity(X)\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_sim[0, 1]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klonQiCR2x3N",
        "outputId": "a43855aa-4d76-4f26-f5de-f60315c91082"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Levenshtein distance\n",
        "!pip install distance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KgUaU1B2-2B",
        "outputId": "91712f34-366f-4ea8-f0a4-0d2723364bc4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting distance\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=8d7652f3656d312e16e7bc33228c44947786446041fdd2bb5ef420a2ca6f3890\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built distance\n",
            "Installing collected packages: distance\n",
            "Successfully installed distance-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import distance\n",
        "\n",
        "text1 = \"The quick brown fox\"\n",
        "text2 = \"A quick brown dog\"\n",
        "\n",
        "levenshtein_distance = distance.levenshtein(text1, text2)\n",
        "\n",
        "print(f\"Levenshtein Distance: {levenshtein_distance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvprTdbC3CK9",
        "outputId": "1f1666ff-01af-463a-b3ac-76db9756d811"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Levenshtein Distance: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flexible String Matching"
      ],
      "metadata": {
        "id": "97CzfXdi3bIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWSN0tl_3jzp",
        "outputId": "b5dbb339-c1e8-44f5-e7fd-30b9c799aefc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "text1 = \"Google LLC.\"\n",
        "text2 = \"Google LLC.\"\n",
        "text3 = \"Apple Inc.\"\n",
        "\n",
        "similarity_score_1_2 = fuzz.ratio(text1, text2)\n",
        "similarity_score_1_3 = fuzz.ratio(text1, text3)\n",
        "\n",
        "print(f\"Similarity between '{text1}' and '{text2}': {similarity_score_1_2}%\")\n",
        "print(f\"Similarity between '{text1}' and '{text3}': {similarity_score_1_3}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzzD-JD13bwX",
        "outputId": "951d6b1d-b9e4-46d9-d0a2-8823153523a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'Google LLC.' and 'Google LLC.': 100%\n",
            "Similarity between 'Google LLC.' and 'Apple Inc.': 38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text summarization"
      ],
      "metadata": {
        "id": "SBf4KfWA36s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4DTBJF874KJz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "TbWoB5XN4RQ7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    return ' '.join(token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and not token.is_punct)\n",
        "\n",
        "\n",
        "\n",
        "def extractive_summarization(text, num_sentences=5):\n",
        "\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "    tokens = nlp(preprocessed_text)\n",
        "    word_freq = Counter(tokens)\n",
        "    sentence_scores = {sentence: sum(word_freq[word] for word in sentence) for sentence in tokens.sents}\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    summary = ' '.join(str(sentence) for sentence in sorted(summary_sentences, key=lambda x: x.start))\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "fbPeioqq4Tcx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "A supercomputer is a computer with a high level of performance as compared to a general-purpose computer.\n",
        "The performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS).\n",
        "Since 2017, supercomputers have existed which can perform over 1017 FLOPS (a hundred quadrillion FLOPS, 100 petaFLOPS or 100 PFLOPS).\n",
        "[3] For comparison, a desktop computer has performance in the range of hundreds of gigaFLOPS (1011) to tens of teraFLOPS (1013).\n",
        "[4][5] Since November 2017, all of the world's fastest 500 supercomputers run on Linux-based operating systems.\n",
        "[6] Additional research is being conducted in the United States, the European Union, Taiwan, Japan, and China to build faster, more powerful and technologically superior exascale supercomputers.\n",
        "\n",
        "Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields, including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of the early moments of the universe, airplane and spacecraft aerodynamics, the detonation of nuclear weapons, and nuclear fusion). They have been essential in the field of cryptanalysis.[8]\n",
        "\n",
        "Supercomputers were introduced in the 1960s, and for several decades the fastest was made by Seymour Cray at Control Data Corporation (CDC), Cray Research and subsequent companies bearing his name or monogram. The first such machines were highly tuned conventional designs that ran more quickly than their more general-purpose contemporaries. Through the decade, increasing amounts of parallelism were added, with one to four processors being typical. In the 1970s, vector processors operating on large arrays of data came to dominate. A notable example is the highly successful Cray-1 of 1976. Vector computers remained the dominant design into the 1990s. From then until today, massively parallel supercomputers with tens of thousands of off-the-shelf processors became the norm.[9][10]\n",
        "\"\"\"\n",
        "\n",
        "summary = extractive_summarization(input_text)\n",
        "\n",
        "print(\"Length of the original content: \", len(input_text))\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)\n",
        "print(\"Length of the summary content: \",len(summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm080H864gdf",
        "outputId": "16f796e6-10ef-4b8f-816c-4862d77cb6c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the original content:  2251\n",
            "\n",
            "Summary:\n",
            "\n",
            " supercomputer computer high level performance compared general purpose computer \n",
            " performance supercomputer commonly measured floating point operations second flops instead million instructions second mips \n",
            " 2017 supercomputers existed perform 1017 flops quadrillion flops 100 petaflops 100 pflops \n",
            " 3 comparison desktop computer performance range hundreds gigaflops 1011 tens teraflops 1013 \n",
            " 4][5 november 2017 world fastest 500 supercomputers run linux based operating systems \n",
            " 6 additional research conducted united states european union taiwan japan china build faster powerful technologically superior exascale supercomputers \n",
            "\n",
            " supercomputers play important role field computational science wide range computationally intensive tasks fields including quantum mechanics weather forecasting climate research oil gas exploration molecular modeling computing structures properties chemical compounds biological macromolecules polymers crystals physical simulations simulations early moments universe airplane spacecraft aerodynamics detonation nuclear weapons nuclear fusion essential field cryptanalysis.[8 \n",
            "\n",
            " supercomputers introduced 1960s decades fastest seymour cray control data corporation cdc cray research subsequent companies bearing monogram machines highly tuned conventional designs ran quickly general purpose contemporaries decade increasing amounts parallelism added processors typical 1970s vector processors operating large arrays data came dominate notable example highly successful cray-1 1976 vector computers remained dominant design 1990s today massively parallel supercomputers tens thousands shelf processors norm.[9][10 \n",
            "\n",
            "Length of the summary content:  1653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "A supercomputer is a computer with a high level of performance as compared to a general-purpose computer.\n",
        "The performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS).\n",
        "Since 2017, supercomputers have existed which can perform over 1017 FLOPS (a hundred quadrillion FLOPS, 100 petaFLOPS or 100 PFLOPS).\n",
        "[3] For comparison, a desktop computer has performance in the range of hundreds of gigaFLOPS (1011) to tens of teraFLOPS (1013).\n",
        "[4][5] Since November 2017, all of the world's fastest 500 supercomputers run on Linux-based operating systems.\n",
        "[6] Additional research is being conducted in the United States, the European Union, Taiwan, Japan, and China to build faster, more powerful and technologically superior exascale supercomputers.\n",
        "\n",
        "Supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields, including quantum mechanics, weather forecasting, climate research, oil and gas exploration, molecular modeling (computing the structures and properties of chemical compounds, biological macromolecules, polymers, and crystals), and physical simulations (such as simulations of the early moments of the universe, airplane and spacecraft aerodynamics, the detonation of nuclear weapons, and nuclear fusion). They have been essential in the field of cryptanalysis.[8]\n",
        "\"\"\"\n",
        "\n",
        "summary = extractive_summarization(input_text)\n",
        "\n",
        "print(\"Length of the original content: \", len(input_text))\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)\n",
        "print(\"Length of the summary content: \",len(summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkk6ifuBG163",
        "outputId": "3cd1e1e2-d7fb-43dc-8981-0d3116afee47"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the original content:  1461\n",
            "\n",
            "Summary:\n",
            "\n",
            " supercomputer computer high level performance compared general purpose computer \n",
            " performance supercomputer commonly measured floating point operations second flops instead million instructions second mips \n",
            " 2017 supercomputers existed perform 1017 flops quadrillion flops 100 petaflops 100 pflops \n",
            " 3 comparison desktop computer performance range hundreds gigaflops 1011 tens teraflops 1013 \n",
            " 4][5 november 2017 world fastest 500 supercomputers run linux based operating systems \n",
            " 6 additional research conducted united states european union taiwan japan china build faster powerful technologically superior exascale supercomputers \n",
            "\n",
            " supercomputers play important role field computational science wide range computationally intensive tasks fields including quantum mechanics weather forecasting climate research oil gas exploration molecular modeling computing structures properties chemical compounds biological macromolecules polymers crystals physical simulations simulations early moments universe airplane spacecraft aerodynamics detonation nuclear weapons nuclear fusion essential field cryptanalysis.[8 \n",
            "\n",
            "Length of the summary content:  1115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yG40KceMKwmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}